{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51f83eae",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/marsggbo/AutoMLDemos/blob/master/ch3/RL-NAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c060740-6b5b-437f-b326-7005d6041cc9",
   "metadata": {},
   "source": [
    "## 1. 搜索空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2747b891-1e25-4f33-b85d-9df56682388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# 定义搜索空间\n",
    "spaces = {\n",
    "    'channels_1': [16, 32, 64],\n",
    "    'channels_2': [16, 32, 64],\n",
    "    'channels_3': [16, 32, 64],\n",
    "    'kernel_size_1': [3, 5, 7],\n",
    "    'kernel_size_2': [3, 5, 7],\n",
    "    'kernel_size_3': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# 随机采样模型编码\n",
    "def sample_encoding():\n",
    "    encoding = [random.choice(space) for space in spaces.values()]\n",
    "    return encoding\n",
    "\n",
    "# 构建模型\n",
    "def build_model(c1, c2, c3, ks1, ks2, ks3):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, c1, kernel_size=ks1, stride=1, padding=ks1//2),\n",
    "        nn.BatchNorm2d(c1), nn.ReLU(),\n",
    "        nn.Conv2d(c1, c2, kernel_size=ks2, stride=2, padding=ks2//2),\n",
    "        nn.BatchNorm2d(c2), nn.ReLU(),\n",
    "        nn.Conv2d(c2, c3, kernel_size=ks3, stride=2, padding=ks3//2),\n",
    "        nn.BatchNorm2d(c3), nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d(1), nn.Flatten(1),\n",
    "        nn.Linear(c3, 10),  # 假设最后一层是10类分类任务\n",
    "    )\n",
    "    \n",
    "# 评估模型性能的函数\n",
    "def evaluate_model(model, num_episodes=10):\n",
    "    # 在这里实现模型评估的代码,例如：计算模型在验证集上的准确率\n",
    "    # 为避免代码复杂，此处返回一个随机数\n",
    "    return torch.rand(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48055394-8ea1-49d7-91c8-724f86839e78",
   "metadata": {},
   "source": [
    "## 2. 策略梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f0e09-371d-4c13-a95b-ae89802f9bbf",
   "metadata": {},
   "source": [
    "### 2.1 策略网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad3cbf6b-497e-452e-9d9a-cf250adcde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_network(input_size, action_space):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_size, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, action_space)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c58745a-923c-4571-b5a8-f044de72478f",
   "metadata": {},
   "source": [
    "### 2.2 定义智能体（策略梯度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f52ce96-ebd8-4534-8095-d0e2d8c3cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(self, state_size, action_size, lr, gamma):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.policy_network = policy_network(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=self.lr)\n",
    "\n",
    "        self.episode_rewards = []\n",
    "        self.episode_log_probs = []\n",
    "\n",
    "    def select_action(self, state):    \n",
    "        state = torch.FloatTensor(state).view(1, -1)\n",
    "        logits = self.policy_network(state)\n",
    "        logits = logits.view(-1, 3)  # 将输出调整为6行3列的形状\n",
    "\n",
    "        action_probs = F.softmax(logits, dim=1)\n",
    "        action_dists = [torch.distributions.Categorical(probs) for probs in action_probs]\n",
    "\n",
    "        actions = [dist.sample() for dist in action_dists]\n",
    "        log_probs = [dist.log_prob(action) for dist, action in zip(action_dists, actions)]\n",
    "\n",
    "        encoding = []\n",
    "        for i in range(self.state_size):\n",
    "            encoding.append(spaces[list(spaces.keys())[i]][actions[i]])\n",
    "\n",
    "        return encoding, torch.stack(log_probs)\n",
    "\n",
    "    def update_policy(self):\n",
    "        R = 0\n",
    "        returns = []\n",
    "        policy_loss = []\n",
    "\n",
    "        for r in self.episode_rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        for log_prob, R in zip(self.episode_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.episode_rewards = []\n",
    "        self.episode_log_probs = []\n",
    "\n",
    "    def add_experience(self, reward, log_prob):\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_log_probs.append(log_prob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "578db1e4-8353-4b95-83ff-974096a36152",
   "metadata": {},
   "source": [
    "## 3. 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4287db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Model encoding = [32, 16, 64, 7, 5, 3]\n",
      "Episode 0: Average reward = 0.46390264987945556\n",
      "Episode 1: Model encoding = [16, 64, 64, 7, 7, 7]\n",
      "Episode 1: Average reward = 0.5476399421691894\n",
      "Episode 2: Model encoding = [32, 32, 16, 5, 3, 7]\n",
      "Episode 2: Average reward = 0.5930303144454956\n",
      "Episode 3: Model encoding = [64, 16, 32, 7, 5, 7]\n",
      "Episode 3: Average reward = 0.4120137870311737\n",
      "Episode 4: Model encoding = [32, 16, 64, 5, 7, 3]\n",
      "Episode 4: Average reward = 0.49418020963668824\n",
      "Episode 5: Model encoding = [32, 32, 16, 5, 5, 3]\n",
      "Episode 5: Average reward = 0.5506968915462493\n",
      "Episode 6: Model encoding = [32, 32, 16, 3, 3, 7]\n",
      "Episode 6: Average reward = 0.501968320608139\n",
      "Episode 7: Model encoding = [16, 32, 32, 7, 3, 3]\n",
      "Episode 7: Average reward = 0.49421594381332395\n",
      "Episode 8: Model encoding = [32, 64, 32, 7, 7, 5]\n",
      "Episode 8: Average reward = 0.43238434433937073\n",
      "Episode 9: Model encoding = [64, 16, 64, 7, 3, 5]\n",
      "Episode 9: Average reward = 0.5080550241470337\n",
      "Episode 10: Model encoding = [64, 16, 32, 7, 3, 3]\n",
      "Episode 10: Average reward = 0.4873189055919647\n",
      "Episode 11: Model encoding = [16, 64, 64, 5, 7, 3]\n",
      "Episode 11: Average reward = 0.49950647592544556\n",
      "Episode 12: Model encoding = [32, 16, 32, 5, 5, 3]\n",
      "Episode 12: Average reward = 0.4938389718532562\n",
      "Episode 13: Model encoding = [32, 64, 32, 5, 5, 7]\n",
      "Episode 13: Average reward = 0.41435012459754944\n",
      "Episode 14: Model encoding = [64, 16, 32, 5, 5, 7]\n",
      "Episode 14: Average reward = 0.513034564256668\n",
      "Episode 15: Model encoding = [64, 64, 32, 7, 7, 3]\n",
      "Episode 15: Average reward = 0.5379389715194702\n",
      "Episode 16: Model encoding = [64, 16, 16, 3, 7, 5]\n",
      "Episode 16: Average reward = 0.4568074929714203\n",
      "Episode 17: Model encoding = [32, 64, 64, 3, 3, 7]\n",
      "Episode 17: Average reward = 0.5662132358551025\n",
      "Episode 18: Model encoding = [32, 64, 16, 7, 7, 7]\n",
      "Episode 18: Average reward = 0.4534637975692749\n",
      "Episode 19: Model encoding = [16, 32, 64, 5, 3, 5]\n",
      "Episode 19: Average reward = 0.5544578325748444\n",
      "\n",
      "Best model encoding: [64, 16, 16, 5, 5, 7]\n",
      "Best model performance: 0.9999048113822937\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    num_episodes = 20\n",
    "    num_steps = 50\n",
    "    lr = 1e-3\n",
    "    gamma = 0.99\n",
    "\n",
    "    agent = PolicyGradient(state_size=len(spaces), action_size=6*3, lr=lr, gamma=gamma)\n",
    "\n",
    "    best_model_encoding = None\n",
    "    best_model_performance = float('-inf')\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = sample_encoding()\n",
    "        print(f'Episode {episode}: Model encoding = {state}')\n",
    "        episode_reward = 0\n",
    "        for _ in range(num_steps):\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            model = build_model(*action)\n",
    "            reward = evaluate_model(model)\n",
    "            agent.add_experience(reward, log_prob)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if reward > best_model_performance:\n",
    "                best_model_performance = reward\n",
    "                best_model_encoding = action\n",
    "\n",
    "            state = action\n",
    "\n",
    "        agent.update_policy()\n",
    "\n",
    "        print(f'Episode {episode}: Average reward = {episode_reward / num_steps}')\n",
    "\n",
    "    print(\"\\nBest model encoding:\", best_model_encoding)\n",
    "    print(\"Best model performance:\", best_model_performance)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79920d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
