{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631015b6",
   "metadata": {},
   "source": [
    "# Illustration of Pruner-Zero\n",
    "\n",
    "**Workflow**:\n",
    "1. Define pruning utilities and search space\n",
    "2. Load model and data\n",
    "3. Run search-based pruning and simple pruning baseline\n",
    "4. Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cc6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from statistics import mean\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "# ============================================================\n",
    "# Math Functions for Search Space\n",
    "# ============================================================\n",
    "\n",
    "def add(x, y):\n",
    "    if isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor):\n",
    "        x, y = torch.broadcast_tensors(x, y)\n",
    "        return x + y\n",
    "    return x + y\n",
    "\n",
    "def sub(x, y):\n",
    "    if isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor):\n",
    "        x, y = torch.broadcast_tensors(x, y)\n",
    "        return x - y\n",
    "    return x - y\n",
    "\n",
    "def mul(x, y):\n",
    "    if isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor):\n",
    "        x, y = torch.broadcast_tensors(x, y)\n",
    "        return x * y\n",
    "    return x * y\n",
    "\n",
    "def div(x, y):\n",
    "    if isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor):\n",
    "        x, y = torch.broadcast_tensors(x, y)\n",
    "        return x / torch.norm(y)\n",
    "    elif isinstance(x, (int, float)) and isinstance(y, (int, float)):\n",
    "        return x / abs(y)\n",
    "    raise TypeError('Input types not supported')\n",
    "\n",
    "def sqr(x):\n",
    "    return x * x if isinstance(x, torch.Tensor) or isinstance(x, (int, float)) else x\n",
    "\n",
    "def neg(x):\n",
    "    return -x\n",
    "\n",
    "def abs_val(x):\n",
    "    return torch.abs(x) if isinstance(x, torch.Tensor) else math.fabs(x)\n",
    "\n",
    "def log(x):\n",
    "    return torch.log(torch.abs(x) + 0.001) if isinstance(x, torch.Tensor) else math.log(abs(x) + 0.001)\n",
    "\n",
    "def sqrt(x):\n",
    "    return torch.sqrt(torch.abs(x)) if isinstance(x, torch.Tensor) else math.sqrt(abs(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return torch.tanh(x) if isinstance(x, torch.Tensor) else math.tanh(x)\n",
    "\n",
    "def pow_op(x):\n",
    "    return torch.pow(x, 2) if isinstance(x, torch.Tensor) else x**2\n",
    "\n",
    "def skp(x):\n",
    "    return x\n",
    "\n",
    "def mms(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return (x - x.min()) / (x.max() - x.min())\n",
    "    return (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "def zsn(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return (x - x.mean()) / x.std()\n",
    "    return (x - mean(x)) / x.std()\n",
    "\n",
    "def exp(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return torch.exp(x.clamp(max=100))\n",
    "    return math.exp(min(x, 100))\n",
    "\n",
    "UNARY_FUNCTIONS = [sqr, neg, abs_val, log, exp, sqrt, tanh, pow_op, skp, mms, zsn]\n",
    "BINARY_FUNCTIONS = [add, sub, mul, div]\n",
    "FUNCTIONS = UNARY_FUNCTIONS + BINARY_FUNCTIONS\n",
    "TERMINALS = ['W', 'G', 'X']\n",
    "\n",
    "FUNCTION_MAP = {func.__name__: func for func in FUNCTIONS}\n",
    "\n",
    "# Genetic Algorithm Parameters\n",
    "PROB_MUTATION = 0.1\n",
    "XO_RATE = 0.8\n",
    "MIN_DEPTH = 2\n",
    "MAX_DEPTH = 4\n",
    "\n",
    "# ============================================================\n",
    "# GPTree Class\n",
    "# ============================================================\n",
    "\n",
    "class GPTree:\n",
    "    def __init__(self, data=None, left=None, right=None):\n",
    "        self.data = data\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def save_tree(self, filename):\n",
    "        tree_data = self._serialize_tree()\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(tree_data, file, indent=4)\n",
    "\n",
    "    def _serialize_tree(self):\n",
    "        data = {'data': self.node_label()}\n",
    "        if self.left:\n",
    "            data['left'] = self.left._serialize_tree()\n",
    "        if self.right:\n",
    "            data['right'] = self.right._serialize_tree()\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_tree(filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            tree_data = json.load(file)\n",
    "        return GPTree._deserialize_tree(tree_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def _deserialize_tree(data):\n",
    "        node = GPTree()\n",
    "        node.data = GPTree._get_function_from_label(data['data'])\n",
    "        if 'left' in data:\n",
    "            node.left = GPTree._deserialize_tree(data['left'])\n",
    "        if 'right' in data:\n",
    "            node.right = GPTree._deserialize_tree(data['right'])\n",
    "        return node\n",
    "\n",
    "    def compute_tree(self, W, G, X):\n",
    "        if self.data in FUNCTIONS:\n",
    "            try:\n",
    "                if self.data in UNARY_FUNCTIONS:\n",
    "                    return self.data(self.left.compute_tree(W, G, X))\n",
    "                else:\n",
    "                    return self.data(\n",
    "                        self.left.compute_tree(W, G, X),\n",
    "                        self.right.compute_tree(W, G, X))\n",
    "            except Exception as e:\n",
    "                # print(f\"Error computing tree: {e}\")\n",
    "                shape = W.shape if isinstance(W, torch.Tensor) else (1, 1)\n",
    "                return torch.zeros(shape, dtype=torch.float32)\n",
    "        elif self.data == 'W':\n",
    "            return W\n",
    "        elif self.data == 'G':\n",
    "            return G\n",
    "        elif self.data == 'X':\n",
    "            return X\n",
    "        else:\n",
    "            shape = W.shape if isinstance(W, torch.Tensor) else G.shape\n",
    "            return torch.full(shape, float(self.data), dtype=torch.float32)\n",
    "\n",
    "    def forward(self, W, G, X):\n",
    "        return self.compute_tree(W, G, X)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_tree(method='grow', max_depth=4, depth=0):\n",
    "        node = GPTree()\n",
    "        if depth >= max_depth:\n",
    "            node.data = TERMINALS[random.randint(0, len(TERMINALS) - 1)]\n",
    "        elif method == 'full' or depth < MIN_DEPTH:\n",
    "            node.data = FUNCTIONS[random.randint(0, len(FUNCTIONS) - 1)]\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                node.data = TERMINALS[random.randint(0, len(TERMINALS) - 1)]\n",
    "            else:\n",
    "                node.data = FUNCTIONS[random.randint(0, len(FUNCTIONS) - 1)]\n",
    "        \n",
    "        if node.data in UNARY_FUNCTIONS:\n",
    "            node.left = GPTree.random_tree(method, max_depth, depth + 1)\n",
    "        elif node.data in FUNCTIONS:\n",
    "            node.left = GPTree.random_tree(method, max_depth, depth + 1)\n",
    "            node.right = GPTree.random_tree(method, max_depth, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def tree_to_string(self):\n",
    "        if self.data in FUNCTIONS:\n",
    "            if self.data in UNARY_FUNCTIONS:\n",
    "                return f\"{self.data.__name__}({self.left.tree_to_string()})\"\n",
    "            else:\n",
    "                return f\"({self.left.tree_to_string()} {self.data.__name__} {self.right.tree_to_string()})\"\n",
    "        return str(self.data)\n",
    "\n",
    "    def size(self):\n",
    "        if self.data in TERMINALS:\n",
    "            return 1\n",
    "        return 1 + (self.left.size() if self.left else 0) + (self.right.size() if self.right else 0)\n",
    "\n",
    "    def depth(self):\n",
    "        return max(self.left.depth() if self.left else 0, self.right.depth() if self.right else 0) + 1\n",
    "\n",
    "    def copy(self):\n",
    "        node = GPTree(self.data)\n",
    "        if self.left: node.left = self.left.copy()\n",
    "        if self.right: node.right = self.right.copy()\n",
    "        return node\n",
    "\n",
    "    def node_label(self):\n",
    "        if self.data in TERMINALS: return str(self.data)\n",
    "        if self.data in FUNCTIONS: return self.data.__name__\n",
    "        return str(self.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_function_from_label(label):\n",
    "        if label in TERMINALS: return label\n",
    "        try: return float(label)\n",
    "        except ValueError: pass\n",
    "        for func in FUNCTIONS:\n",
    "            if func.__name__ == label: return func\n",
    "        raise ValueError(f\"Unknown label: {label}\")\n",
    "        \n",
    "    def scan_tree(self, count, replacement=None):\n",
    "        count[0] -= 1\n",
    "        if count[0] <= 0:\n",
    "            if replacement is None:\n",
    "                return self.copy() # Return copy to avoid ref issues\n",
    "            else:\n",
    "                self.data = replacement.data\n",
    "                self.left = replacement.left\n",
    "                self.right = replacement.right\n",
    "        else:\n",
    "            if self.left and count[0] > 0: self.left.scan_tree(count, replacement)\n",
    "            if self.right and count[0] > 0: self.right.scan_tree(count, replacement)\n",
    "\n",
    "    def mutation(self):\n",
    "        if random.random() < PROB_MUTATION:\n",
    "            new_tree = GPTree.random_tree(method='grow', max_depth=2)\n",
    "            self.data = new_tree.data\n",
    "            self.left = new_tree.left\n",
    "            self.right = new_tree.right\n",
    "        elif self.left:\n",
    "            self.left.mutation()\n",
    "        elif self.right:\n",
    "            self.right.mutation()\n",
    "\n",
    "    def crossover(self, other):\n",
    "        if random.random() < XO_RATE:\n",
    "            count = [random.randint(1, other.size())]\n",
    "            second = other.scan_tree(count.copy(), None)\n",
    "            if second:\n",
    "                 count = [random.randint(1, self.size())]\n",
    "                 self.scan_tree(count, second)\n",
    "        elif self.left:\n",
    "            self.left.crossover(other)\n",
    "        elif self.right:\n",
    "            self.right.crossover(other)\n",
    "\n",
    "# ============================================================\n",
    "# Helper Classes and Functions\n",
    "# ============================================================\n",
    "\n",
    "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "    try:\n",
    "        traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    except:\n",
    "        print(\"Failed to load wikitext-2 remote, trying local or skipping...\")\n",
    "        # Add fallback logic if needed\n",
    "        raise\n",
    "        \n",
    "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
    "    \n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader\n",
    "\n",
    "def find_layers(module, layers=[nn.Linear], name=''):\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(find_layers(\n",
    "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
    "        ))\n",
    "    return res\n",
    "\n",
    "def check_sparsity(model):\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "    count = 0\n",
    "    total_params = 0\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "        for name in subset:\n",
    "            W = subset[name].weight.data\n",
    "            count += (W == 0).sum().item()\n",
    "            total_params += W.numel()\n",
    "    model.config.use_cache = use_cache\n",
    "    return float(count) / total_params\n",
    "\n",
    "class WrappedGPT:\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.dev = self.layer.weight.device\n",
    "        self.columns = layer.weight.data.shape[1]\n",
    "        self.scaler_row = torch.zeros((self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        self.scaler_row *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        inp = inp.type(torch.float32)\n",
    "        self.scaler_row += torch.norm(inp, p=2, dim=1) ** 2 / self.nsamples\n",
    "\n",
    "def prepare_calibration_input(model, dataloader, device):\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    if hasattr(model, 'hf_device_map') and \"model.embed_tokens\" in model.hf_device_map:\n",
    "        device = model.hf_device_map[\"model.embed_tokens\"]\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros((len(dataloader), model.seqlen, model.config.hidden_size), dtype=dtype, device=device)\n",
    "    inps.requires_grad = False\n",
    "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps[cache['i']] = inp\n",
    "            cache['i'] += 1\n",
    "            cache['attention_mask'] = kwargs.get('attention_mask')\n",
    "            cache['position_ids'] = kwargs.get('position_ids')\n",
    "            raise ValueError\n",
    "        def __getattr__(self, name):\n",
    "            try:\n",
    "                return super().__getattr__(name)\n",
    "            except AttributeError:\n",
    "                return getattr(self.module, name)\n",
    "\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch[0].to(device))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    attention_mask = cache['attention_mask']\n",
    "    position_ids = cache['position_ids']\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    return inps, outs, attention_mask, position_ids\n",
    "\n",
    "# ============================================================\n",
    "# Main Pruning Function\n",
    "# ============================================================\n",
    "\n",
    "def apply_pruning(model, engine, sparsity_ratio, dataloader=None, nsamples=128, seed=0, tokenizer=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    if dataloader is None:\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided if dataloader is not.\")\n",
    "        # Default to wikitext2 if no dataloader provided\n",
    "        dataloader = get_wikitext2(nsamples, seed, model.seqlen, tokenizer)\n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device)\n",
    "\n",
    "    layers = model.model.layers\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        if hasattr(model, 'hf_device_map') and f\"model.layers.{i}\" in model.hf_device_map:\n",
    "            dev = model.hf_device_map[f\"model.layers.{i}\"]\n",
    "            inps = inps.to(dev)\n",
    "            outs = outs.to(dev)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(dev)\n",
    "            if position_ids is not None:\n",
    "                position_ids = position_ids.to(dev)\n",
    "\n",
    "        wrapped_layers = {}\n",
    "        for name in subset:\n",
    "            wrapped_layers[name] = WrappedGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        for name in wrapped_layers:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "\n",
    "        for j in range(len(dataloader)):\n",
    "            with torch.no_grad():\n",
    "                layer_args = {}\n",
    "\n",
    "                if attention_mask is not None:\n",
    "                    if attention_mask.size(0) == len(dataloader):\n",
    "                        layer_args['attention_mask'] = attention_mask[j].unsqueeze(0)\n",
    "                    else:\n",
    "                        layer_args['attention_mask'] = attention_mask\n",
    "\n",
    "                pos_ids = None\n",
    "                if position_ids is not None:\n",
    "                    if position_ids.size(0) == len(dataloader):\n",
    "                        pos_ids = position_ids[j].unsqueeze(0)\n",
    "                    else:\n",
    "                        pos_ids = position_ids\n",
    "                    layer_args['position_ids'] = pos_ids\n",
    "\n",
    "                if hasattr(model.model, \"rotary_emb\") and pos_ids is not None:\n",
    "                    try:\n",
    "                        layer_args[\"position_embeddings\"] = model.model.rotary_emb(\n",
    "                            inps[j].unsqueeze(0), pos_ids\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        layer_args[\"position_embeddings\"] = model.model.rotary_emb(pos_ids)\n",
    "\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), **layer_args)[0]\n",
    "\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        for name in subset:\n",
    "            # print(f\"Pruning layer {i} name {name}\")\n",
    "            W = torch.abs(subset[name].weight.data)\n",
    "            X = wrapped_layers[name].scaler_row.reshape((1, -1))\n",
    "            G = torch.ones_like(W) # Dummy gradient\n",
    "\n",
    "            W_metric = engine.forward(\n",
    "                W.to(dtype=torch.float32),\n",
    "                G.to(device=W.device, dtype=torch.float32),\n",
    "                X.to(device=W.device, dtype=torch.float32),\n",
    "            )\n",
    "\n",
    "            sort_res = torch.sort(W_metric, dim=-1, stable=True)\n",
    "            indices = sort_res[1][:, :int(W_metric.shape[1] * sparsity_ratio)]\n",
    "            W_mask = torch.zeros_like(W_metric, dtype=torch.bool)\n",
    "            W_mask.scatter_(1, indices, True)\n",
    "\n",
    "            subset[name].weight.data[W_mask] = 0\n",
    "\n",
    "        for j in range(len(dataloader)):\n",
    "            with torch.no_grad():\n",
    "                layer_args = {}\n",
    "                if attention_mask is not None:\n",
    "                    if attention_mask.size(0) == len(dataloader):\n",
    "                        layer_args['attention_mask'] = attention_mask[j].unsqueeze(0)\n",
    "                    else:\n",
    "                        layer_args['attention_mask'] = attention_mask\n",
    "\n",
    "                pos_ids = None\n",
    "                if position_ids is not None:\n",
    "                    if position_ids.size(0) == len(dataloader):\n",
    "                        pos_ids = position_ids[j].unsqueeze(0)\n",
    "                    else:\n",
    "                        pos_ids = position_ids\n",
    "                    layer_args['position_ids'] = pos_ids\n",
    "\n",
    "                if hasattr(model.model, \"rotary_emb\") and pos_ids is not None:\n",
    "                    try:\n",
    "                        layer_args[\"position_embeddings\"] = model.model.rotary_emb(\n",
    "                            inps[j].unsqueeze(0), pos_ids\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        layer_args[\"position_embeddings\"] = model.model.rotary_emb(pos_ids)\n",
    "\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), **layer_args)[0]\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    torch.cuda.empty_cache()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2545e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Environment Check\n",
      "============================================================\n",
      "Python version: 3.12.12 (main, Oct  9 2025, 11:07:00) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "PyTorch version: 2.10.0\n",
      "CUDA available: False\n",
      "============================================================\n",
      "✓ transformers is installed\n",
      "✓ datasets is installed\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Start Testing\n",
      "============================================================\n",
      "\n",
      "Step 1: Loading model Qwen/Qwen2.5-0.5B\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbddbb370c64ef88f6b73288392becd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "  Sequence length: 512\n",
      "  Device: cpu\n",
      "\n",
      "Step 2: Evaluating original model\n",
      "------------------------------------------------------------\n",
      "Evaluating model perplexity...\n",
      "Loading WikiText-2 dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32df86c578aa47e4a51798bdc6e67cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9effb7e1c5fb48b6ae7f6bc1d4b22294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e6811e113d469fb088c16123e3f044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967628b05e804965b722426a7a218fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e47a08969b04f008fcdd5a1402a080f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c593aa4c8347f88030c9f5e9b7be50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70396ea150b7439b9333ff7b17ade93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981a10463ba94ef6b1fd3d58c434c1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0b6b073cab44888cdd65a944498e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1741ec6ad0e64cc0b226a63c032762f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd27bdd60738491aa1c1274c202e01c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9621cd446da04e718ee94d10d2ed7e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2541000 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluation samples: 584\n",
      "  Sample 0/584\n",
      "  Sample 50/584\n",
      "  Sample 100/584\n",
      "  Sample 150/584\n",
      "  Sample 200/584\n",
      "  Sample 250/584\n",
      "  Sample 300/584\n",
      "  Sample 350/584\n",
      "  Sample 400/584\n",
      "  Sample 450/584\n",
      "  Sample 500/584\n",
      "  Sample 550/584\n",
      "✓ Original model perplexity: 17.1919\n",
      "\n",
      "Step 3: Applying pruning\n",
      "------------------------------------------------------------\n",
      "Creating pruning expression...\n",
      "Pruning expression: (W mul sqrt(X))\n",
      "✓ Pruning completed\n",
      "  Target sparsity: 10.0%\n",
      "  Actual sparsity: 9.95%\n",
      "\n",
      "Step 3b: Applying simple pruning baseline\n",
      "------------------------------------------------------------\n",
      "Applying pruning (target sparsity: 10.0%)...\n",
      "  Pruning layer 0 - self_attn.q_proj\n",
      "  Pruning layer 0 - self_attn.k_proj\n",
      "  Pruning layer 0 - self_attn.v_proj\n",
      "  Pruning layer 0 - self_attn.o_proj\n",
      "  Pruning layer 0 - mlp.gate_proj\n",
      "  Pruning layer 0 - mlp.up_proj\n",
      "  Pruning layer 0 - mlp.down_proj\n",
      "  Pruning layer 1 - self_attn.q_proj\n",
      "  Pruning layer 1 - self_attn.k_proj\n",
      "  Pruning layer 1 - self_attn.v_proj\n",
      "  Pruning layer 1 - self_attn.o_proj\n",
      "  Pruning layer 1 - mlp.gate_proj\n",
      "  Pruning layer 1 - mlp.up_proj\n",
      "  Pruning layer 1 - mlp.down_proj\n",
      "  Pruning layer 2 - self_attn.q_proj\n",
      "  Pruning layer 2 - self_attn.k_proj\n",
      "  Pruning layer 2 - self_attn.v_proj\n",
      "  Pruning layer 2 - self_attn.o_proj\n",
      "  Pruning layer 2 - mlp.gate_proj\n",
      "  Pruning layer 2 - mlp.up_proj\n",
      "  Pruning layer 2 - mlp.down_proj\n",
      "  Pruning layer 3 - self_attn.q_proj\n",
      "  Pruning layer 3 - self_attn.k_proj\n",
      "  Pruning layer 3 - self_attn.v_proj\n",
      "  Pruning layer 3 - self_attn.o_proj\n",
      "  Pruning layer 3 - mlp.gate_proj\n",
      "  Pruning layer 3 - mlp.up_proj\n",
      "  Pruning layer 3 - mlp.down_proj\n",
      "  Pruning layer 4 - self_attn.q_proj\n",
      "  Pruning layer 4 - self_attn.k_proj\n",
      "  Pruning layer 4 - self_attn.v_proj\n",
      "  Pruning layer 4 - self_attn.o_proj\n",
      "  Pruning layer 4 - mlp.gate_proj\n",
      "  Pruning layer 4 - mlp.up_proj\n",
      "  Pruning layer 4 - mlp.down_proj\n",
      "  Pruning layer 5 - self_attn.q_proj\n",
      "  Pruning layer 5 - self_attn.k_proj\n",
      "  Pruning layer 5 - self_attn.v_proj\n",
      "  Pruning layer 5 - self_attn.o_proj\n",
      "  Pruning layer 5 - mlp.gate_proj\n",
      "  Pruning layer 5 - mlp.up_proj\n",
      "  Pruning layer 5 - mlp.down_proj\n",
      "  Pruning layer 6 - self_attn.q_proj\n",
      "  Pruning layer 6 - self_attn.k_proj\n",
      "  Pruning layer 6 - self_attn.v_proj\n",
      "  Pruning layer 6 - self_attn.o_proj\n",
      "  Pruning layer 6 - mlp.gate_proj\n",
      "  Pruning layer 6 - mlp.up_proj\n",
      "  Pruning layer 6 - mlp.down_proj\n",
      "  Pruning layer 7 - self_attn.q_proj\n",
      "  Pruning layer 7 - self_attn.k_proj\n",
      "  Pruning layer 7 - self_attn.v_proj\n",
      "  Pruning layer 7 - self_attn.o_proj\n",
      "  Pruning layer 7 - mlp.gate_proj\n",
      "  Pruning layer 7 - mlp.up_proj\n",
      "  Pruning layer 7 - mlp.down_proj\n",
      "  Pruning layer 8 - self_attn.q_proj\n",
      "  Pruning layer 8 - self_attn.k_proj\n",
      "  Pruning layer 8 - self_attn.v_proj\n",
      "  Pruning layer 8 - self_attn.o_proj\n",
      "  Pruning layer 8 - mlp.gate_proj\n",
      "  Pruning layer 8 - mlp.up_proj\n",
      "  Pruning layer 8 - mlp.down_proj\n",
      "  Pruning layer 9 - self_attn.q_proj\n",
      "  Pruning layer 9 - self_attn.k_proj\n",
      "  Pruning layer 9 - self_attn.v_proj\n",
      "  Pruning layer 9 - self_attn.o_proj\n",
      "  Pruning layer 9 - mlp.gate_proj\n",
      "  Pruning layer 9 - mlp.up_proj\n",
      "  Pruning layer 9 - mlp.down_proj\n",
      "  Pruning layer 10 - self_attn.q_proj\n",
      "  Pruning layer 10 - self_attn.k_proj\n",
      "  Pruning layer 10 - self_attn.v_proj\n",
      "  Pruning layer 10 - self_attn.o_proj\n",
      "  Pruning layer 10 - mlp.gate_proj\n",
      "  Pruning layer 10 - mlp.up_proj\n",
      "  Pruning layer 10 - mlp.down_proj\n",
      "  Pruning layer 11 - self_attn.q_proj\n",
      "  Pruning layer 11 - self_attn.k_proj\n",
      "  Pruning layer 11 - self_attn.v_proj\n",
      "  Pruning layer 11 - self_attn.o_proj\n",
      "  Pruning layer 11 - mlp.gate_proj\n",
      "  Pruning layer 11 - mlp.up_proj\n",
      "  Pruning layer 11 - mlp.down_proj\n",
      "  Pruning layer 12 - self_attn.q_proj\n",
      "  Pruning layer 12 - self_attn.k_proj\n",
      "  Pruning layer 12 - self_attn.v_proj\n",
      "  Pruning layer 12 - self_attn.o_proj\n",
      "  Pruning layer 12 - mlp.gate_proj\n",
      "  Pruning layer 12 - mlp.up_proj\n",
      "  Pruning layer 12 - mlp.down_proj\n",
      "  Pruning layer 13 - self_attn.q_proj\n",
      "  Pruning layer 13 - self_attn.k_proj\n",
      "  Pruning layer 13 - self_attn.v_proj\n",
      "  Pruning layer 13 - self_attn.o_proj\n",
      "  Pruning layer 13 - mlp.gate_proj\n",
      "  Pruning layer 13 - mlp.up_proj\n",
      "  Pruning layer 13 - mlp.down_proj\n",
      "  Pruning layer 14 - self_attn.q_proj\n",
      "  Pruning layer 14 - self_attn.k_proj\n",
      "  Pruning layer 14 - self_attn.v_proj\n",
      "  Pruning layer 14 - self_attn.o_proj\n",
      "  Pruning layer 14 - mlp.gate_proj\n",
      "  Pruning layer 14 - mlp.up_proj\n",
      "  Pruning layer 14 - mlp.down_proj\n",
      "  Pruning layer 15 - self_attn.q_proj\n",
      "  Pruning layer 15 - self_attn.k_proj\n",
      "  Pruning layer 15 - self_attn.v_proj\n",
      "  Pruning layer 15 - self_attn.o_proj\n",
      "  Pruning layer 15 - mlp.gate_proj\n",
      "  Pruning layer 15 - mlp.up_proj\n",
      "  Pruning layer 15 - mlp.down_proj\n",
      "  Pruning layer 16 - self_attn.q_proj\n",
      "  Pruning layer 16 - self_attn.k_proj\n",
      "  Pruning layer 16 - self_attn.v_proj\n",
      "  Pruning layer 16 - self_attn.o_proj\n",
      "  Pruning layer 16 - mlp.gate_proj\n",
      "  Pruning layer 16 - mlp.up_proj\n",
      "  Pruning layer 16 - mlp.down_proj\n",
      "  Pruning layer 17 - self_attn.q_proj\n",
      "  Pruning layer 17 - self_attn.k_proj\n",
      "  Pruning layer 17 - self_attn.v_proj\n",
      "  Pruning layer 17 - self_attn.o_proj\n",
      "  Pruning layer 17 - mlp.gate_proj\n",
      "  Pruning layer 17 - mlp.up_proj\n",
      "  Pruning layer 17 - mlp.down_proj\n",
      "  Pruning layer 18 - self_attn.q_proj\n",
      "  Pruning layer 18 - self_attn.k_proj\n",
      "  Pruning layer 18 - self_attn.v_proj\n",
      "  Pruning layer 18 - self_attn.o_proj\n",
      "  Pruning layer 18 - mlp.gate_proj\n",
      "  Pruning layer 18 - mlp.up_proj\n",
      "  Pruning layer 18 - mlp.down_proj\n",
      "  Pruning layer 19 - self_attn.q_proj\n",
      "  Pruning layer 19 - self_attn.k_proj\n",
      "  Pruning layer 19 - self_attn.v_proj\n",
      "  Pruning layer 19 - self_attn.o_proj\n",
      "  Pruning layer 19 - mlp.gate_proj\n",
      "  Pruning layer 19 - mlp.up_proj\n",
      "  Pruning layer 19 - mlp.down_proj\n",
      "  Pruning layer 20 - self_attn.q_proj\n",
      "  Pruning layer 20 - self_attn.k_proj\n",
      "  Pruning layer 20 - self_attn.v_proj\n",
      "  Pruning layer 20 - self_attn.o_proj\n",
      "  Pruning layer 20 - mlp.gate_proj\n",
      "  Pruning layer 20 - mlp.up_proj\n",
      "  Pruning layer 20 - mlp.down_proj\n",
      "  Pruning layer 21 - self_attn.q_proj\n",
      "  Pruning layer 21 - self_attn.k_proj\n",
      "  Pruning layer 21 - self_attn.v_proj\n",
      "  Pruning layer 21 - self_attn.o_proj\n",
      "  Pruning layer 21 - mlp.gate_proj\n",
      "  Pruning layer 21 - mlp.up_proj\n",
      "  Pruning layer 21 - mlp.down_proj\n",
      "  Pruning layer 22 - self_attn.q_proj\n",
      "  Pruning layer 22 - self_attn.k_proj\n",
      "  Pruning layer 22 - self_attn.v_proj\n",
      "  Pruning layer 22 - self_attn.o_proj\n",
      "  Pruning layer 22 - mlp.gate_proj\n",
      "  Pruning layer 22 - mlp.up_proj\n",
      "  Pruning layer 22 - mlp.down_proj\n",
      "  Pruning layer 23 - self_attn.q_proj\n",
      "  Pruning layer 23 - self_attn.k_proj\n",
      "  Pruning layer 23 - self_attn.v_proj\n",
      "  Pruning layer 23 - self_attn.o_proj\n",
      "  Pruning layer 23 - mlp.gate_proj\n",
      "  Pruning layer 23 - mlp.up_proj\n",
      "  Pruning layer 23 - mlp.down_proj\n",
      "✓ Simple pruning completed\n",
      "  Target sparsity: 10.0%\n",
      "  Actual sparsity: 10.03%\n",
      "\n",
      "Step 4: Evaluating pruned models\n",
      "------------------------------------------------------------\n",
      "Evaluating model perplexity...\n",
      "Loading WikiText-2 dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b3f74d044a439ca787456dbaafc537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b66176e2f640cd8e2c459378fe763f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a009ee697c440984fc60897f8fc519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57b5300d25d40aea6c0e6d229348e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43e46f2b8e74ccbacc1e04dc732e95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8075a53186f4addb0ac33d4451d6cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00cfc0108694b44bcde844754403739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e8b47d1bfb4abeaf8bfb2f057782cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e0b1f0bf00446e97ca0f56253b05a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e17258ba0eb418eb1c1dfa5902bf083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae4b559343045768c4d8dcf76ac4ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a9928ddd9e46d7872f1e9b4f94ebc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluation samples: 584\n",
      "  Sample 0/584\n",
      "  Sample 50/584\n",
      "  Sample 100/584\n",
      "  Sample 150/584\n",
      "  Sample 200/584\n",
      "  Sample 250/584\n",
      "  Sample 300/584\n",
      "  Sample 350/584\n",
      "  Sample 400/584\n",
      "  Sample 450/584\n",
      "  Sample 500/584\n",
      "  Sample 550/584\n",
      "✓ Search-based perplexity: 17.2372\n",
      "Evaluating model perplexity...\n",
      "Loading WikiText-2 dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98399c8ef7ce4032919233b7dcb70e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3536aa3619ed4c03a7d94edb1bf4a2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac35da9bc3641518dca32c6eef52927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75154a43e4743ba9484c7a619262cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50947a982c8d450a8ae621864bbe1057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fa8188cd874055a32fe672d5c3321b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec20404397c46d1b75c72290cd589ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d10a5301124319940ae2f184623af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a002f430c004f1f9ce6e24b599b7079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3b57bf52ad4cd780a5edaf967910a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5a82376ee0448694ef024dfe21f97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce11a4525df4af1a24e32d81d799eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluation samples: 584\n",
      "  Sample 0/584\n",
      "  Sample 50/584\n",
      "  Sample 100/584\n",
      "  Sample 150/584\n",
      "  Sample 200/584\n",
      "  Sample 250/584\n",
      "  Sample 300/584\n",
      "  Sample 350/584\n",
      "  Sample 400/584\n",
      "  Sample 450/584\n",
      "  Sample 500/584\n",
      "  Sample 550/584\n",
      "✓ Simple magnitude perplexity: 17.4497\n",
      "\n",
      "============================================================\n",
      "Summary\n",
      "============================================================\n",
      "Original model perplexity:   17.1919\n",
      "\n",
      "Comparison Table\n",
      "------------------------------------------------------------\n",
      "Method                       PPL        ΔPPL     Growth%   Sparsity%\n",
      "------------------------------------------------------------\n",
      "Search-based             17.2372      0.0453       0.26%       9.95%\n",
      "Simple magnitude         17.4497      0.2578       1.50%      10.03%\n",
      "============================================================\n",
      "\n",
      "Saving pruned model to: pruned_qwen2.5_0.5b_sparsity0.1.pt\n",
      "✓ Done!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simplified LLM pruning test script\n",
    "Tests pruning on the Qwen2.5-0.5B model.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import pruner_utils\n",
    "\n",
    "# ============================================================\n",
    "# Environment check\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try importing required packages\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    print(\"✓ transformers is installed\")\n",
    "except ImportError as e:\n",
    "    print(\"✗ transformers is NOT installed\")\n",
    "    print(f\"  Error: {e}\")\n",
    "    print(\"  Please run: pip install transformers\")\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    print(\"✓ datasets is installed\")\n",
    "except ImportError as e:\n",
    "    print(\"✗ datasets is NOT installed\")\n",
    "    print(f\"  Error: {e}\")\n",
    "    print(\"  Please run: pip install datasets\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "SPARSITY_RATIO = 0.1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# Simplified data loading\n",
    "# ============================================================\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "\n",
    "\n",
    "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "    \"\"\"Load the WikiText-2 dataset.\"\"\"\n",
    "    print(\"Loading WikiText-2 dataset...\")\n",
    "    from datasets import load_dataset\n",
    "    import random\n",
    "\n",
    "    traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\", download_mode='force_redownload')\n",
    "    testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\",  download_mode='force_redownload')\n",
    "\n",
    "    # Encode datasets\n",
    "    trainenc = tokenizer(\" \".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    # Generate samples\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader, testenc\n",
    "\n",
    "\n",
    "def find_layers(module, layers=[nn.Linear], name=\"\"):\n",
    "    \"\"\"Recursively find layers of specific types.\"\"\"\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(\n",
    "            find_layers(\n",
    "                child, layers=layers, name=name + \".\" + name1 if name != \"\" else name1\n",
    "            )\n",
    "        )\n",
    "    return res\n",
    "\n",
    "\n",
    "def check_sparsity(model):\n",
    "    \"\"\"Check model sparsity (fraction of zero weights in Linear layers).\"\"\"\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    layers = model.model.layers\n",
    "    zero_count = 0\n",
    "    total_params = 0\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        subset = find_layers(layers[i])\n",
    "        for name in subset:\n",
    "            W = subset[name].weight.data\n",
    "            zero_count += (W == 0).sum().item()\n",
    "            total_params += W.numel()\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    return float(zero_count) / total_params\n",
    "\n",
    "\n",
    "def eval_ppl_wikitext(model, testenc, bs=1, device=None):\n",
    "    \"\"\"Evaluate perplexity on WikiText.\"\"\"\n",
    "    testenc = testenc.input_ids\n",
    "    nsamples = testenc.numel() // model.seqlen\n",
    "\n",
    "    nlls = []\n",
    "    print(f\"Number of evaluation samples: {nsamples}\")\n",
    "\n",
    "    for i in range(0, nsamples, bs):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  Sample {i}/{nsamples}\")\n",
    "\n",
    "        j = min(i + bs, nsamples)\n",
    "\n",
    "        # Prepare inputs\n",
    "        inputs = testenc[:, (i * model.seqlen):(j * model.seqlen)].to(device)\n",
    "        inputs = inputs.reshape(j - i, model.seqlen)\n",
    "\n",
    "        # Forward pass\n",
    "        lm_logits = model(inputs).logits\n",
    "\n",
    "        # Compute loss\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(\n",
    "            shift_logits.reshape(-1, shift_logits.size(-1)),\n",
    "            shift_labels.reshape(-1),\n",
    "        )\n",
    "\n",
    "        # Negative log-likelihood\n",
    "        neg_log_likelihood = loss.float() * model.seqlen * (j - i)\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "    return ppl.item()\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, device=None):\n",
    "    \"\"\"Evaluate model perplexity.\"\"\"\n",
    "    print(\"Evaluating model perplexity...\")\n",
    "    _, testloader = get_wikitext2(128, 0, model.seqlen, tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ppl_test = eval_ppl_wikitext(model, testloader, bs=1, device=device)\n",
    "\n",
    "    return ppl_test\n",
    "\n",
    "\n",
    "def apply_simple_pruning(model, sparsity_ratio=0.1):\n",
    "    \"\"\"Simple magnitude pruning (sets smallest-magnitude weights to zero).\"\"\"\n",
    "    print(f\"Applying pruning (target sparsity: {sparsity_ratio * 100}%)...\")\n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    layers = model.model.layers\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        for name in subset:\n",
    "            print(f\"  Pruning layer {i} - {name}\")\n",
    "            W = subset[name].weight.data\n",
    "\n",
    "            # Magnitude as pruning metric\n",
    "            W_metric = torch.abs(W)\n",
    "\n",
    "            # Threshold by global fraction within this matrix\n",
    "            sort_res = torch.sort(W_metric.flatten(), stable=True)\n",
    "            threshold = sort_res[0][int(W_metric.numel() * sparsity_ratio)]\n",
    "            W_mask = W_metric <= threshold\n",
    "\n",
    "            # Zero out weights\n",
    "            subset[name].weight.data[W_mask] = 0\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    torch.cuda.empty_cache()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Start Testing\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Load model\n",
    "    print(f\"\\nStep 1: Loading model {MODEL_NAME}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            # force_download=True,\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            # force_download=True,\n",
    "        )\n",
    "        model = model.to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        model.seqlen = 512\n",
    "        print(\"✓ Model loaded successfully\")\n",
    "        print(f\"  Sequence length: {model.seqlen}\")\n",
    "        print(f\"  Device: {DEVICE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Model loading failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Evaluate original model\n",
    "    print(\"\\nStep 2: Evaluating original model\")\n",
    "    print(\"-\" * 60)\n",
    "    try:\n",
    "        original_ppl = evaluate_model(model, tokenizer, device=DEVICE)\n",
    "        print(f\"✓ Original model perplexity: {original_ppl:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Step 3: Apply pruning\n",
    "    print(\"\\nStep 3: Applying pruning\")\n",
    "    print(\"-\" * 60)\n",
    "    results = []\n",
    "    try:\n",
    "        import copy\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        \n",
    "        # Search-based pruning integration\n",
    "        print(\"Creating pruning expression...\")\n",
    "        # Create a simple expression: mul(W, sqrt(X))\n",
    "        simple_tree = pruner_utils.GPTree(pruner_utils.mul)\n",
    "        simple_tree.left = pruner_utils.GPTree('W')\n",
    "        simple_tree.right = pruner_utils.GPTree(pruner_utils.sqrt)\n",
    "        simple_tree.right.left = pruner_utils.GPTree('X')\n",
    "        print(f\"Pruning expression: {simple_tree.tree_to_string()}\")\n",
    "\n",
    "        # Apply pruning using pruner_utils\n",
    "        model_copy = pruner_utils.apply_pruning(\n",
    "            model_copy,\n",
    "            simple_tree,\n",
    "            sparsity_ratio=SPARSITY_RATIO,\n",
    "            tokenizer=tokenizer,\n",
    "            device=DEVICE,\n",
    "            nsamples=32  # Reduced samples for faster testing\n",
    "        )\n",
    "        \n",
    "        # model_copy = apply_simple_pruning(model_copy, SPARSITY_RATIO)\n",
    "        model_copy.eval()\n",
    "\n",
    "        actual_sparsity = check_sparsity(model_copy)\n",
    "        print(\"✓ Pruning completed\")\n",
    "        print(f\"  Target sparsity: {SPARSITY_RATIO * 100:.1f}%\")\n",
    "        print(f\"  Actual sparsity: {actual_sparsity * 100:.2f}%\")\n",
    "        results.append({\n",
    "            \"name\": \"Search-based\",\n",
    "            \"model\": model_copy,\n",
    "            \"sparsity\": actual_sparsity,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Pruning failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Step 3b: Simple pruning baseline\n",
    "    print(\"\\nStep 3b: Applying simple pruning baseline\")\n",
    "    print(\"-\" * 60)\n",
    "    try:\n",
    "        import copy\n",
    "        simple_copy = copy.deepcopy(model)\n",
    "        simple_copy = apply_simple_pruning(simple_copy, SPARSITY_RATIO)\n",
    "        simple_copy.eval()\n",
    "\n",
    "        simple_sparsity = check_sparsity(simple_copy)\n",
    "        print(\"✓ Simple pruning completed\")\n",
    "        print(f\"  Target sparsity: {SPARSITY_RATIO * 100:.1f}%\")\n",
    "        print(f\"  Actual sparsity: {simple_sparsity * 100:.2f}%\")\n",
    "        results.append({\n",
    "            \"name\": \"Simple magnitude\",\n",
    "            \"model\": simple_copy,\n",
    "            \"sparsity\": simple_sparsity,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Simple pruning failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Step 4: Evaluate pruned models\n",
    "    print(\"\\nStep 4: Evaluating pruned models\")\n",
    "    print(\"-\" * 60)\n",
    "    try:\n",
    "        for item in results:\n",
    "            item[\"ppl\"] = evaluate_model(item[\"model\"], tokenizer, device=DEVICE)\n",
    "            print(f\"✓ {item['name']} perplexity: {item['ppl']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Step 5: Summarize results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original model perplexity:   {original_ppl:.4f}\")\n",
    "    print(\"\\nComparison Table\")\n",
    "    print(\"-\" * 60)\n",
    "    header = f\"{'Method':<20}{'PPL':>12}{'ΔPPL':>12}{'Growth%':>12}{'Sparsity%':>12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 60)\n",
    "    for item in results:\n",
    "        delta = item[\"ppl\"] - original_ppl\n",
    "        growth = (item[\"ppl\"] / original_ppl - 1) * 100\n",
    "        row = f\"{item['name']:<20}{item['ppl']:>12.4f}{delta:>12.4f}{growth:>11.2f}%{item['sparsity'] * 100:>11.2f}%\"\n",
    "        print(row)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Save model\n",
    "    save_path = f\"pruned_qwen2.5_0.5b_sparsity{SPARSITY_RATIO}.pt\"\n",
    "    print(f\"\\nSaving pruned model to: {save_path}\")\n",
    "    torch.save(model_copy.state_dict(), save_path)\n",
    "    print(\"✓ Done!\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18a967-eeb5-4e78-b10a-aaf4b77414af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1cfd9-081f-4f90-8b00-7083c6a9fc74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
